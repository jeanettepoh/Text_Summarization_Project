Tokenizer:
  input_max_length: 1024
  input_truncation: True
  target_max_length: 128
  target_truncation: True


TrainingArguments:
  num_train_epochs: 0.5  
  warmup_steps: 500
  per_device_train_batch_size: 1  
  weight_decay: 0.01
  logging_steps: 100  
  evaluation_strategy: "steps"
  eval_steps: 1000  
  save_steps: 1000000  
  gradient_accumulation_steps: 8 


ModelEvaluation:
  batch_size: 16
  input_max_length: 1024
  input_truncation: True
  length_penalty: 0.8
  num_beams: 8
  target_max_length: 128